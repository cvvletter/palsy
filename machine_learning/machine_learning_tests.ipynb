{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning tests are run here\n",
    "Some code blocks always have to be run, some can be skipped if only certain tests are of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports (mandatory)\n",
    "We start with importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the dataset (mandatory)\n",
    "Here, we make the dataset that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the features\n",
    "dftrain = np.load('features.npy')\n",
    "dftrain = dftrain[:,17:135] # discard some peripheral patients and the chin landmarks\n",
    "# load the corresponding labels\n",
    "y_train = np.load('labels.npy')\n",
    "# y_train = y_train[50:202] # discard the same peripheral patients\n",
    "\n",
    "dftrain = dftrain / 900.0\n",
    "\n",
    "classes = ['peripheral palsy', 'central palsy', 'healthy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the dataset (optional)\n",
    "See the dataset that we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (203, 118)\n",
      "Labels shape: (203,)\n"
     ]
    }
   ],
   "source": [
    "# print(\"Features:\", dftrain[0])\n",
    "# print(\"Labels:\", classes[y_train[0]])\n",
    "print(\"Features shape:\",dftrain.shape)\n",
    "print(\"Labels shape:\",y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 (optional)\n",
    "This is the first experiment. A DNN is made with many different amounts of hidden layers and nodes. The final accuracy is printed using LOOCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy = 0.5073891625615764\n"
     ]
    }
   ],
   "source": [
    "# use leave-one-out cross-validation to test the accuracy of the model\n",
    "correct = 0\n",
    "n = len(y_train)\n",
    "for i in range(len(y_train)):\n",
    "    # print(\"Run\", i, \"of\", len(dftrain))\n",
    "    x_test = dftrain[i]\n",
    "    x_test = np.reshape(x_test, (1,-1))\n",
    "    y_test = y_train[i]\n",
    "    x_train_loocv = np.delete(dftrain, i, 0)\n",
    "    y_train_loocv = np.delete(y_train, i, 0)\n",
    "    firstmodel = tf.keras.Sequential([\n",
    "        # tf.keras.layers.Dense(136, activation='relu'),\n",
    "        tf.keras.layers.Dense(108800, activation='relu'),\n",
    "        # tf.keras.layers.Dense(10880, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    firstmodel.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(),\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics = tf.keras.metrics.CategoricalAccuracy(),\n",
    "    )\n",
    "    firstmodel.fit(x_train_loocv, y_train_loocv, epochs=1, verbose=0, batch_size=len(x_train_loocv), shuffle=True)\n",
    "    prediction = np.argmax(firstmodel.predict(x_test))\n",
    "    # print(\"Prediction:\", classes[prediction.astype(int)])\n",
    "    # print(\"Actual:    \", classes[y_test.astype(int)])\n",
    "    if (prediction == y_test):\n",
    "        correct += 1\n",
    "    #     print(\"Prediction is correct!\")\n",
    "    # else:\n",
    "    #     print(\"Prediction not correct.\")\n",
    "    n += 1\n",
    "print(\"Final accuracy =\", correct/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 (optional)\n",
    "This is the second experiment. A support vector machine (SVM) will be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy = 0.8078817733990148\n"
     ]
    }
   ],
   "source": [
    "# use leave-one-out cross-validation to test the accuracy of the model\n",
    "correct = 0\n",
    "n = len(y_train)\n",
    "for i in range(len(y_train)):\n",
    "    x_test = dftrain[i]\n",
    "    x_test = np.reshape(x_test, ([1,-1]))\n",
    "    y_test = y_train[i]\n",
    "    x_train_loocv = np.delete(dftrain, i, 0)\n",
    "    y_train_loocv = np.delete(y_train, i, 0)\n",
    "    model2 = svm.SVC(kernel='poly',degree=5)\n",
    "    model2.fit(x_train_loocv, y_train_loocv)\n",
    "    prediction = model2.predict(x_test)\n",
    "    if (prediction == y_test):\n",
    "        correct += 1\n",
    "print(\"Final accuracy =\", correct/n)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
